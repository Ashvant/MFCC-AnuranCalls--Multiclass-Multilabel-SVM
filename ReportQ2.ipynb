{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Number: 8569202378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name: Ashvant Ram Selvam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Download the Anuran Calls (MFCCs) Data Set from: https://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29#. Choose 70% of the data randomly as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MFCCs_ 1  MFCCs_ 2  MFCCs_ 3  MFCCs_ 4  MFCCs_ 5  MFCCs_ 6  MFCCs_ 7  \\\n",
      "0          1.0  0.152936 -0.105586  0.200722  0.317201  0.260764  0.100945   \n",
      "1          1.0  0.171534 -0.098975  0.268425  0.338672  0.268353  0.060835   \n",
      "2          1.0  0.152317 -0.082973  0.287128  0.276014  0.189867  0.008714   \n",
      "3          1.0  0.224392  0.118985  0.329432  0.372088  0.361005  0.015501   \n",
      "4          1.0  0.087817 -0.068345  0.306967  0.330923  0.249144  0.006884   \n",
      "5          1.0  0.099704 -0.033408  0.349895  0.344535  0.247569  0.022407   \n",
      "6          1.0  0.021676 -0.062075  0.318229  0.380439  0.179043 -0.041667   \n",
      "7          1.0  0.145130 -0.033660  0.284166  0.279537  0.175211  0.005791   \n",
      "8          1.0  0.271326  0.027777  0.375738  0.385432  0.272457  0.098192   \n",
      "9          1.0  0.120565 -0.107235  0.316555  0.364437  0.307757  0.025992   \n",
      "10         1.0  0.148539 -0.096910  0.257523  0.260881  0.312603  0.134134   \n",
      "11         1.0  0.277948  0.091657  0.331656  0.307372  0.257359  0.065702   \n",
      "12         1.0  0.106109 -0.025790  0.358875  0.297543  0.244335  0.016446   \n",
      "13         1.0  0.126523 -0.040482  0.341129  0.381446  0.261154 -0.017049   \n",
      "14         1.0  0.267687  0.099327  0.510454  0.511468  0.317788  0.067992   \n",
      "15         1.0  0.137623 -0.085808  0.322446  0.344695  0.285642  0.056517   \n",
      "16         1.0  0.263944  0.090358  0.368888  0.356645  0.252806  0.063921   \n",
      "17         1.0  0.146299 -0.075174  0.291935  0.367094  0.268947  0.054049   \n",
      "18         1.0  0.179298 -0.038306  0.319636  0.383029  0.275313  0.099083   \n",
      "19         1.0  0.273218 -0.234703 -0.079620  0.159811  0.416406  0.368838   \n",
      "20         1.0  0.196429  0.009021  0.317772  0.293484  0.185684  0.044063   \n",
      "21         1.0  0.230999  0.135657  0.431966  0.403423  0.276571  0.060464   \n",
      "22         1.0  0.145109 -0.035846  0.282707  0.291044  0.206862  0.048627   \n",
      "23         1.0  0.235682  0.029241  0.349117  0.355932  0.290697  0.081008   \n",
      "24         1.0  0.146944 -0.009583  0.352534  0.313435  0.197599  0.015352   \n",
      "25         1.0  0.233512  0.067249  0.352310  0.316899  0.220584  0.044433   \n",
      "26         1.0  0.172672 -0.037870  0.301100  0.303533  0.203767  0.017798   \n",
      "27         1.0  0.198494  0.078718  0.478231  0.425219  0.257916  0.049410   \n",
      "28         1.0  0.165998 -0.004175  0.289963  0.295084  0.224001  0.085586   \n",
      "29         1.0  0.155225 -0.063337  0.231699  0.284514  0.219596  0.038581   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "7165       1.0  0.132365  0.503936  0.271392 -0.042392  0.024487  0.080317   \n",
      "7166       1.0  0.165383  0.408082  0.270187  0.015300  0.019000  0.025284   \n",
      "7167       1.0  0.411916  0.322796  0.344183  0.333873  0.094867 -0.230982   \n",
      "7168       1.0  0.404936  0.726247  0.167376 -0.169260  0.112154  0.050805   \n",
      "7169       1.0  0.209224  0.625373  0.246565 -0.108078  0.174846  0.100347   \n",
      "7170       1.0  0.242842  0.516606  0.263976 -0.040676  0.052257  0.055201   \n",
      "7171       1.0  0.153789  0.522557  0.267617 -0.043499  0.034436  0.081305   \n",
      "7172       1.0  0.266711  0.735734  0.233772 -0.075867  0.140193  0.080212   \n",
      "7173       1.0  0.183156  0.506970  0.247162 -0.048503  0.041090  0.093071   \n",
      "7174       1.0  0.197679  0.553775  0.269271 -0.056261  0.062528  0.061367   \n",
      "7175       1.0 -0.673025 -0.279960  0.065945  0.086890  0.469024  0.088019   \n",
      "7176       1.0 -0.574239 -0.258302  0.007644  0.078566  0.402132  0.093501   \n",
      "7177       1.0 -0.515567 -0.219812  0.118473  0.106919  0.471155  0.121196   \n",
      "7178       1.0 -0.614409 -0.261315  0.051138  0.062119  0.501500  0.096506   \n",
      "7179       1.0 -0.578837 -0.331501  0.105921  0.089931  0.456617  0.079131   \n",
      "7180       1.0 -0.528595 -0.208051  0.103669  0.086537  0.408476  0.069610   \n",
      "7181       1.0 -0.442139 -0.328404  0.031452  0.056017  0.424856  0.073288   \n",
      "7182       1.0 -0.616029 -0.302357  0.063417  0.095671  0.439930  0.069414   \n",
      "7183       1.0 -0.547168 -0.266780  0.056115  0.048947  0.423631  0.081924   \n",
      "7184       1.0 -0.520958 -0.258779 -0.070416 -0.025129  0.447967  0.180033   \n",
      "7185       1.0 -0.512794  0.056322  0.259677  0.030140  0.369783 -0.117154   \n",
      "7186       1.0 -0.591520 -0.268901  0.050042  0.116960  0.444706  0.059268   \n",
      "7187       1.0 -0.507564 -0.249969  0.031781 -0.079888  0.484274  0.125143   \n",
      "7188       1.0 -0.512599 -0.171956  0.325813  0.169600  0.421567 -0.123749   \n",
      "7189       1.0 -0.558546 -0.238442  0.066527  0.123090  0.395953  0.066522   \n",
      "7190       1.0 -0.554504 -0.337717  0.035533  0.034511  0.443451  0.093889   \n",
      "7191       1.0 -0.517273 -0.370574  0.030673  0.068097  0.402890  0.096628   \n",
      "7192       1.0 -0.582557 -0.343237  0.029468  0.064179  0.385596  0.114905   \n",
      "7193       1.0 -0.519497 -0.307553 -0.004922  0.072865  0.377131  0.086866   \n",
      "7194       1.0 -0.508833 -0.324106  0.062068  0.078211  0.397188  0.094596   \n",
      "\n",
      "      MFCCs_ 8  MFCCs_ 9  MFCCs_10    ...     MFCCs_17  MFCCs_18  MFCCs_19  \\\n",
      "0    -0.150063 -0.171128  0.124676    ...    -0.108351 -0.077623 -0.009568   \n",
      "1    -0.222475 -0.207693  0.170883    ...    -0.090974 -0.056510 -0.035303   \n",
      "2    -0.242234 -0.219153  0.232538    ...    -0.050691 -0.023590 -0.066722   \n",
      "3    -0.194347 -0.098181  0.270375    ...    -0.136009 -0.177037 -0.130498   \n",
      "4    -0.265423 -0.172700  0.266434    ...    -0.048885 -0.053074 -0.088550   \n",
      "5    -0.213767 -0.127916  0.277353    ...    -0.080487 -0.130089 -0.171478   \n",
      "6    -0.252300 -0.167117  0.220027    ...    -0.046620 -0.055146 -0.085972   \n",
      "7    -0.183329 -0.158483  0.192567    ...    -0.055978 -0.048219 -0.056637   \n",
      "8    -0.173730 -0.157857  0.207181    ...    -0.120723 -0.112607 -0.156933   \n",
      "9    -0.294179 -0.223236  0.268435    ...    -0.051073 -0.052568 -0.111338   \n",
      "10   -0.216262 -0.189334  0.261960    ...    -0.034082 -0.120716 -0.100800   \n",
      "11   -0.191860 -0.133537  0.220020    ...    -0.119167 -0.110900 -0.112485   \n",
      "12   -0.288733 -0.146731  0.314207    ...    -0.062939 -0.071182 -0.066827   \n",
      "13   -0.294064 -0.222278  0.282338    ...    -0.071544 -0.060630 -0.067230   \n",
      "14   -0.202826 -0.142236  0.235510    ...    -0.138830 -0.139922 -0.126448   \n",
      "15   -0.314418 -0.252324  0.288897    ...    -0.058694 -0.072913 -0.064263   \n",
      "16   -0.155007 -0.137743  0.200262    ...    -0.074168 -0.083995 -0.104413   \n",
      "17   -0.242952 -0.232617  0.235722    ...    -0.051154 -0.038580 -0.022396   \n",
      "18   -0.207998 -0.219215  0.182845    ...    -0.110969 -0.105833 -0.115237   \n",
      "19    0.016878 -0.171288 -0.115424    ...    -0.253103 -0.154244 -0.002606   \n",
      "20   -0.169936 -0.121461  0.237437    ...    -0.045409 -0.067118 -0.047625   \n",
      "21   -0.192200 -0.187348  0.180486    ...    -0.163367 -0.170739 -0.169508   \n",
      "22   -0.172111 -0.115698  0.251256    ...    -0.044077 -0.067219 -0.058514   \n",
      "23   -0.193793 -0.151462  0.212130    ...    -0.158932 -0.098565 -0.078413   \n",
      "24   -0.216492 -0.133865  0.278309    ...    -0.043595 -0.052536 -0.024106   \n",
      "25   -0.172653 -0.127641  0.190017    ...    -0.105466 -0.070941 -0.085853   \n",
      "26   -0.197260 -0.124021  0.256757    ...    -0.078277 -0.023172  0.002809   \n",
      "27   -0.146034 -0.127461  0.170725    ...    -0.172904 -0.164822 -0.152341   \n",
      "28   -0.180608 -0.169064  0.221722    ...    -0.067536 -0.084953 -0.116397   \n",
      "29   -0.183926 -0.108442  0.245208    ...    -0.072937 -0.018316 -0.034315   \n",
      "...        ...       ...       ...    ...          ...       ...       ...   \n",
      "7165  0.112706 -0.071058 -0.114414    ...    -0.006677 -0.070338  0.003964   \n",
      "7166  0.120438  0.042605 -0.053960    ...     0.007785 -0.002231  0.020593   \n",
      "7167  0.276750  0.232506 -0.372219    ...     0.029924  0.031286 -0.038367   \n",
      "7168  0.078670 -0.243258 -0.120933    ...     0.090483  0.126233  0.015162   \n",
      "7169  0.035056 -0.149125 -0.099348    ...    -0.041610 -0.039468  0.086045   \n",
      "7170  0.125361 -0.023455 -0.064192    ...     0.004518 -0.013307 -0.001609   \n",
      "7171  0.113272 -0.034906 -0.055563    ...    -0.005648 -0.028727  0.031281   \n",
      "7172  0.119675 -0.038130 -0.033289    ...    -0.027436 -0.007999  0.058854   \n",
      "7173  0.109081 -0.049473 -0.044991    ...    -0.008959 -0.034239  0.036489   \n",
      "7174  0.094837 -0.047792 -0.063015    ...     0.021719 -0.041396 -0.013619   \n",
      "7175 -0.197498  0.031180  0.096919    ...     0.052054  0.114138  0.069353   \n",
      "7176 -0.146350  0.040779  0.105841    ...     0.062850  0.038051  0.019184   \n",
      "7177 -0.136494  0.027486  0.106110    ...     0.049024  0.109761  0.043604   \n",
      "7178 -0.155093  0.035657  0.121933    ...     0.048454  0.072420  0.017823   \n",
      "7179 -0.133406  0.056485  0.093265    ...     0.079075  0.125759  0.044132   \n",
      "7180 -0.155217  0.080287  0.127136    ...     0.053063  0.085869  0.020705   \n",
      "7181 -0.140148  0.043070  0.087675    ...     0.024943  0.068279  0.015953   \n",
      "7182 -0.145534  0.031354  0.069073    ...     0.056971  0.037285  0.023818   \n",
      "7183 -0.184252  0.024682  0.111803    ...     0.049831  0.115522  0.051333   \n",
      "7184 -0.062297  0.032410 -0.005379    ...     0.027392  0.019266 -0.057772   \n",
      "7185 -0.189292  0.248948  0.193566    ...     0.096589  0.255261  0.047039   \n",
      "7186 -0.158389  0.066648  0.083924    ...     0.074444  0.117996  0.045522   \n",
      "7187 -0.069555  0.114265  0.099647    ...     0.011524  0.111998  0.015886   \n",
      "7188 -0.298284  0.089382  0.243902    ...     0.021225  0.157321  0.042847   \n",
      "7189 -0.152216  0.078294  0.094184    ...     0.034202  0.040540 -0.003755   \n",
      "7190 -0.100753  0.037087  0.081075    ...     0.069430  0.071001  0.021591   \n",
      "7191 -0.116460  0.063727  0.089034    ...     0.061127  0.068978  0.017745   \n",
      "7192 -0.103317  0.070370  0.081317    ...     0.082474  0.077771 -0.009688   \n",
      "7193 -0.115799  0.056979  0.089316    ...     0.051796  0.069073  0.017963   \n",
      "7194 -0.117672  0.058874  0.076180    ...     0.061455  0.072983 -0.003980   \n",
      "\n",
      "      MFCCs_20  MFCCs_21  MFCCs_22           Family      Genus  \\\n",
      "0     0.057684  0.118680  0.014038  Leptodactylidae  Adenomera   \n",
      "1     0.020140  0.082263  0.029056  Leptodactylidae  Adenomera   \n",
      "2    -0.025083  0.099108  0.077162  Leptodactylidae  Adenomera   \n",
      "3    -0.054766 -0.018691  0.023954  Leptodactylidae  Adenomera   \n",
      "4    -0.031346  0.108610  0.079244  Leptodactylidae  Adenomera   \n",
      "5    -0.071569  0.077643  0.064903  Leptodactylidae  Adenomera   \n",
      "6    -0.009127  0.065630  0.044040  Leptodactylidae  Adenomera   \n",
      "7    -0.022419  0.070085  0.021419  Leptodactylidae  Adenomera   \n",
      "8    -0.118527 -0.002471  0.002304  Leptodactylidae  Adenomera   \n",
      "9    -0.040014  0.090204  0.088025  Leptodactylidae  Adenomera   \n",
      "10   -0.001992  0.111462  0.103637  Leptodactylidae  Adenomera   \n",
      "11   -0.053184  0.044291 -0.011456  Leptodactylidae  Adenomera   \n",
      "12   -0.028048  0.058353  0.064368  Leptodactylidae  Adenomera   \n",
      "13   -0.038196  0.070127  0.048440  Leptodactylidae  Adenomera   \n",
      "14   -0.067570  0.057888 -0.011998  Leptodactylidae  Adenomera   \n",
      "15    0.022455  0.130752  0.074132  Leptodactylidae  Adenomera   \n",
      "16   -0.071431  0.028842  0.019180  Leptodactylidae  Adenomera   \n",
      "17   -0.018891  0.051480  0.031871  Leptodactylidae  Adenomera   \n",
      "18   -0.012819  0.083194  0.052101  Leptodactylidae  Adenomera   \n",
      "19    0.092999  0.091724  0.003595  Leptodactylidae  Adenomera   \n",
      "20   -0.005875  0.053107  0.030669  Leptodactylidae  Adenomera   \n",
      "21   -0.112446  0.065072  0.050254  Leptodactylidae  Adenomera   \n",
      "22   -0.001358  0.082058  0.045492  Leptodactylidae  Adenomera   \n",
      "23   -0.043410  0.033478 -0.006953  Leptodactylidae  Adenomera   \n",
      "24    0.016081  0.062506  0.042285  Leptodactylidae  Adenomera   \n",
      "25   -0.025053  0.055194  0.003098  Leptodactylidae  Adenomera   \n",
      "26    0.009133  0.063916  0.047634  Leptodactylidae  Adenomera   \n",
      "27   -0.068851  0.024412 -0.009821  Leptodactylidae  Adenomera   \n",
      "28   -0.008499  0.101151  0.022322  Leptodactylidae  Adenomera   \n",
      "29   -0.029563  0.051715  0.023925  Leptodactylidae  Adenomera   \n",
      "...        ...       ...       ...              ...        ...   \n",
      "7165  0.032534  0.002975  0.002724          Hylidae     Scinax   \n",
      "7166  0.014499  0.008208  0.018023          Hylidae     Scinax   \n",
      "7167 -0.007837  0.102445  0.000755          Hylidae     Scinax   \n",
      "7168  0.059531  0.050905 -0.026956          Hylidae     Scinax   \n",
      "7169  0.071944 -0.021574  0.036031          Hylidae     Scinax   \n",
      "7170  0.022170  0.014327  0.003001          Hylidae     Scinax   \n",
      "7171  0.033687  0.005242  0.014982          Hylidae     Scinax   \n",
      "7172  0.008365  0.019086  0.090131          Hylidae     Scinax   \n",
      "7173  0.042205  0.003017  0.005602          Hylidae     Scinax   \n",
      "7174  0.010337  0.017903  0.022108          Hylidae     Scinax   \n",
      "7175  0.050519 -0.069281 -0.106642          Hylidae     Scinax   \n",
      "7176  0.027745 -0.109677 -0.131287          Hylidae     Scinax   \n",
      "7177  0.047369 -0.018168 -0.085380          Hylidae     Scinax   \n",
      "7178 -0.021418 -0.094280 -0.091600          Hylidae     Scinax   \n",
      "7179  0.017652 -0.077831 -0.132563          Hylidae     Scinax   \n",
      "7180  0.009959 -0.072003 -0.121693          Hylidae     Scinax   \n",
      "7181  0.058530 -0.009746 -0.080940          Hylidae     Scinax   \n",
      "7182  0.042104 -0.035603 -0.097343          Hylidae     Scinax   \n",
      "7183  0.019697 -0.061764 -0.078648          Hylidae     Scinax   \n",
      "7184  0.037710  0.047717  0.001436          Hylidae     Scinax   \n",
      "7185 -0.016643 -0.046598 -0.055862          Hylidae     Scinax   \n",
      "7186  0.050734 -0.034757 -0.085131          Hylidae     Scinax   \n",
      "7187  0.008636 -0.021933 -0.069692          Hylidae     Scinax   \n",
      "7188  0.006852  0.005439 -0.013693          Hylidae     Scinax   \n",
      "7189  0.036059 -0.031853 -0.090206          Hylidae     Scinax   \n",
      "7190  0.052449 -0.021860 -0.079860          Hylidae     Scinax   \n",
      "7191  0.046461 -0.015418 -0.101892          Hylidae     Scinax   \n",
      "7192  0.027834 -0.000531 -0.080425          Hylidae     Scinax   \n",
      "7193  0.041803 -0.027911 -0.096895          Hylidae     Scinax   \n",
      "7194  0.031560 -0.029355 -0.087910          Hylidae     Scinax   \n",
      "\n",
      "             Species  RecordID  \n",
      "0     AdenomeraAndre         1  \n",
      "1     AdenomeraAndre         1  \n",
      "2     AdenomeraAndre         1  \n",
      "3     AdenomeraAndre         1  \n",
      "4     AdenomeraAndre         1  \n",
      "5     AdenomeraAndre         1  \n",
      "6     AdenomeraAndre         1  \n",
      "7     AdenomeraAndre         1  \n",
      "8     AdenomeraAndre         1  \n",
      "9     AdenomeraAndre         1  \n",
      "10    AdenomeraAndre         1  \n",
      "11    AdenomeraAndre         1  \n",
      "12    AdenomeraAndre         1  \n",
      "13    AdenomeraAndre         1  \n",
      "14    AdenomeraAndre         1  \n",
      "15    AdenomeraAndre         1  \n",
      "16    AdenomeraAndre         1  \n",
      "17    AdenomeraAndre         1  \n",
      "18    AdenomeraAndre         1  \n",
      "19    AdenomeraAndre         1  \n",
      "20    AdenomeraAndre         1  \n",
      "21    AdenomeraAndre         1  \n",
      "22    AdenomeraAndre         1  \n",
      "23    AdenomeraAndre         1  \n",
      "24    AdenomeraAndre         1  \n",
      "25    AdenomeraAndre         1  \n",
      "26    AdenomeraAndre         1  \n",
      "27    AdenomeraAndre         1  \n",
      "28    AdenomeraAndre         1  \n",
      "29    AdenomeraAndre         1  \n",
      "...              ...       ...  \n",
      "7165     ScinaxRuber        59  \n",
      "7166     ScinaxRuber        59  \n",
      "7167     ScinaxRuber        59  \n",
      "7168     ScinaxRuber        59  \n",
      "7169     ScinaxRuber        59  \n",
      "7170     ScinaxRuber        59  \n",
      "7171     ScinaxRuber        59  \n",
      "7172     ScinaxRuber        59  \n",
      "7173     ScinaxRuber        59  \n",
      "7174     ScinaxRuber        59  \n",
      "7175     ScinaxRuber        60  \n",
      "7176     ScinaxRuber        60  \n",
      "7177     ScinaxRuber        60  \n",
      "7178     ScinaxRuber        60  \n",
      "7179     ScinaxRuber        60  \n",
      "7180     ScinaxRuber        60  \n",
      "7181     ScinaxRuber        60  \n",
      "7182     ScinaxRuber        60  \n",
      "7183     ScinaxRuber        60  \n",
      "7184     ScinaxRuber        60  \n",
      "7185     ScinaxRuber        60  \n",
      "7186     ScinaxRuber        60  \n",
      "7187     ScinaxRuber        60  \n",
      "7188     ScinaxRuber        60  \n",
      "7189     ScinaxRuber        60  \n",
      "7190     ScinaxRuber        60  \n",
      "7191     ScinaxRuber        60  \n",
      "7192     ScinaxRuber        60  \n",
      "7193     ScinaxRuber        60  \n",
      "7194     ScinaxRuber        60  \n",
      "\n",
      "[7195 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "fileData = pd.read_csv(\"Frogs_MFCCs.csv\")\n",
    "print(fileData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold, GridSearchCV\n",
    "train, test = train_test_split(fileData, test_size=.3)\n",
    "trainLabel = pd.DataFrame(train[[\"Family\",\"Species\",\"Genus\"]])\n",
    "trainData = train.drop([\"Family\", \"Genus\", \"Species\", \"RecordID\"], axis=1)\n",
    "testLabel = pd.DataFrame(test[[\"Family\",\"Species\",\"Genus\"]])\n",
    "testData = test.drop([\"Family\", \"Genus\", \"Species\", \"RecordID\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Each instance has three labels: Families, Genus, and Species. Each of the labels\n",
    "has multiple classes. We wish to solve a multi-class and multi-label problem.\n",
    "One of the most important approaches to multi-class classification is to train a\n",
    "classifier for each label. We first try this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Research exact match and hamming score/ loss methods for evaluating multilabel\n",
    "classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact Match:\n",
    "Exact Match loss is a very strict parameter which considers the sample correctly classified only if all the three labels are classified correctly. Any partial correct answer is considered as wrong classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming Loss: Hamming loss is a less strict parameter which takes into account partial correct answers. Hence, hamming loss usually gives a lower value error value compared to exact match loss as partial correct answers are taken into account and not completely classified as wrong classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation. You are welcome to try to solve the problem with both normalized and raw attributes and report the\n",
    "results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "trainDataNonormalize = trainData\n",
    "trainData = normalize(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "def fit_gaussian(trainDataGaussian, trainLabelGaussian,classEstimated):\n",
    "    sig = np.array([value for value in range(1,20,2)])/10\n",
    "    width = 1/(2*sig*sig)\n",
    "    params = {'estimator__C':(10**np.array([value for value in range(0,6)]))/100, 'estimator__gamma':width}\n",
    "    print(classEstimated)\n",
    "    svc = SVC()\n",
    "    classifier = GridSearchCV(OneVsRestClassifier(svc), params, cv=10)\n",
    "    classifier.fit(trainDataGaussian, trainLabelGaussian)\n",
    "    print(\"Best Weight of SVM penalty   = \", classifier.best_params_['estimator__C'])\n",
    "    print(\"Best Width of Gaussian Kernel   = \", classifier.best_params_['estimator__gamma'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "Best Weight of SVM penalty   =  1000.0\n",
      "Best Width of Gaussian Kernel   =  2.0\n",
      "Genus\n",
      "Best Weight of SVM penalty   =  10.0\n",
      "Best Width of Gaussian Kernel   =  5.55555555556\n",
      "Species\n",
      "Best Weight of SVM penalty   =  10.0\n",
      "Best Width of Gaussian Kernel   =  2.0\n"
     ]
    }
   ],
   "source": [
    "svc_family = fit_gaussian(trainData, trainLabel[\"Family\"],\"Family\")\n",
    "svc_genus = fit_gaussian(trainData, trainLabel[\"Genus\"],\"Genus\")\n",
    "svc_species = fit_gaussian(trainData, trainLabel[\"Species\"],\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the cross validated parameters obtained for each of the svms's constructed for the specific class labels. These SVMs will be used to calculate the hamming and exact match loss for the normalized train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hamming_loss(trueLabels,predictedLabels):\n",
    "    total=0\n",
    "    for trueRow,predictedRow in zip(trueLabels,predictedLabels):\n",
    "        correct=0\n",
    "        for a,b in zip(trueRow,predictedRow):\n",
    "            if(a==b):\n",
    "                correct+=1\n",
    "        ratioOfCorrect = correct/len(trueRow)\n",
    "        total+=ratioOfCorrect\n",
    "    return total/len(trueLabels)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exactMatch_loss(trueLabels,predictedLabels):\n",
    "    total=0\n",
    "    for trueRow,predictedRow in zip(trueLabels,predictedLabels):\n",
    "        correct=1\n",
    "        for a,b in zip(trueRow,predictedRow):\n",
    "            if(a!=b):\n",
    "                correct=0\n",
    "        total+=correct\n",
    "    return total/len(trueLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss :  0.010189902732746225\n",
      "Exact Match Loss     :  0.016674386289949106\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([svc_family.predict(testData), svc_species.predict(testData), svc_genus.predict(testData)]).T\n",
    "print(\"Hamming Loss : \", 1-hamming_loss(np.array(testLabel), predicted))\n",
    "print(\"Exact Match Loss     : \", 1-exactMatch_loss(np.array(testLabel), predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the exact match loss is higher compared to hamming loss since exact match loss is strict measure which considers partially correct classifications as wrong classifications while calculating accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Vs ALL Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_linear_OneVsRest(trainDataLinearOvR, trainLabelLinearOvR,classEstimated):\n",
    "    params = {'estimator__C':(10**np.array([value for value in range(0,6)]))/100}\n",
    "    print(classEstimated)\n",
    "    svc = LinearSVC(penalty='l1', dual = False)\n",
    "    classifier = GridSearchCV(OneVsRestClassifier(svc, n_jobs=-1), params, cv=10)\n",
    "    classifier.fit(trainDataLinearOvR, trainLabelLinearOvR)\n",
    "    print(\"Best Weight of SVM penalty   = \", classifier.best_params_['estimator__C'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "Best Weight of SVM penalty   =  10.0\n",
      "Genus\n",
      "Best Weight of SVM penalty   =  100.0\n",
      "Species\n",
      "Best Weight of SVM penalty   =  100.0\n"
     ]
    }
   ],
   "source": [
    "svc_family = fit_linear_OneVsRest(trainData, trainLabel[\"Family\"],\"Family\")\n",
    "svc_genus = fit_linear_OneVsRest(trainData, trainLabel[\"Genus\"],\"Genus\")\n",
    "svc_species = fit_linear_OneVsRest(trainData, trainLabel[\"Species\"],\"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss :  0.054809325304924994\n",
      "Exact Match Loss     :  0.08661417322834641\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([svc_family.predict(testData), svc_species.predict(testData), svc_genus.predict(testData)]).T\n",
    "print(\"Hamming Loss : \", 1-hamming_loss(np.array(testLabel), predicted))\n",
    "print(\"Exact Match Loss     : \", 1-exactMatch_loss(np.array(testLabel), predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Repeat 2(b)ii with L1-penalized SVMs. Remember to normalize the attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes are already normalized in the data preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_linear(trainDataLinear, trainLabelLinear,classEstimated):\n",
    "    params = {'C':(10**np.array([value for value in range(0,6)]))/100}\n",
    "    print(classEstimated)\n",
    "    svc = LinearSVC(penalty='l1', dual = False)\n",
    "    classifier = GridSearchCV(svc, params, cv=10)\n",
    "    classifier.fit(trainDataLinear, trainLabelLinear)\n",
    "    print(\"Best Weight of SVM penalty   = \", classifier.best_params_['C'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "Best Weight of SVM penalty   =  100.0\n",
      "Genus\n",
      "Best Weight of SVM penalty   =  100.0\n",
      "Species\n",
      "Best Weight of SVM penalty   =  1000.0\n"
     ]
    }
   ],
   "source": [
    "svc_family = fit_linear(trainData, trainLabel[\"Family\"],\"Family\")\n",
    "svc_genus = fit_linear(trainData, trainLabel[\"Genus\"],\"Genus\")\n",
    "svc_species = fit_linear(trainData, trainLabel[\"Species\"],\"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Loss : 0.08707735062528954\n",
      "Hamming Loss     : 0.05496371777057263\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([svc_family.predict(testData), svc_species.predict(testData), svc_genus.predict(testData)]).T\n",
    "print(\"Exact Match Loss : \", 1-exactMatch_loss(np.array(testLabel), predicted))\n",
    "print(\"Hamming Loss     : \", 1-hamming_loss(np.array(testLabel), predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Repeat 2(b)iii by using SMOTE or any other method you know to remedy\n",
    "class imbalance. Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "trainDataSMOTE_family, trainLabelSMOTE_family = sm.fit_sample(trainData, trainLabel[\"Family\"])\n",
    "sm = SMOTE(random_state=42)\n",
    "trainDataSMOTE_genus, trainLabelSMOTE_genus = sm.fit_sample(trainData, trainLabel[\"Genus\"])\n",
    "sm = SMOTE(random_state=42)\n",
    "trainDataSMOTE_species, trainLabelSMOTE_species = sm.fit_sample(trainData, trainLabel[\"Species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "Best Weight of SVM penalty   = 10\n",
      "Best Width of Gaussian Kernel   = 2.0\n",
      "Genus\n",
      "Best Weight of SVM penalty   = 100\n",
      "Best Width of Gaussian Kernel   = 5.55555555556\n",
      "Species\n",
      "Best Weight of SVM penalty   = 10\n",
      "Best Width of Gaussian Kernel   = 2.0\n"
     ]
    }
   ],
   "source": [
    "svc_family = fit_gaussian(trainDataSMOTE_family, trainLabelSMOTE_family, \"Family\")\n",
    "svc_genus = fit_gaussian(trainDataSMOTE_genus, trainLabelSMOTE_genus, \"Genus\")\n",
    "svc_species = fit_gaussian(trainDataSMOTE_species, trainLabelSMOTE_species, \"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss : 0.00812783696155708\n",
      "Exact Match Loss     : 0.01345530338119498\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([svc_family.predict(testData), svc_species.predict(testData), svc_genus.predict(testData)]).T\n",
    "print(\"Hamming Loss     : \", 1-hamming_loss(np.array(testLabel), predicted))\n",
    "print(\"Exact Match Loss     : \", 1-exactMatch_loss(np.array(testLabel), predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family\n",
      "Best Weight of SVM penalty   = 1000\n",
      "Genus\n",
      "Best Weight of SVM penalty   = 10\n",
      "Species\n",
      "Best Weight of SVM penalty   = 100\n"
     ]
    }
   ],
   "source": [
    "svc_family = fit_linear(trainDataSMOTE_family, trainLabelSMOTE_family, \"Family\")\n",
    "svc_genus = fit_linear(trainDataSMOTE_genus, trainLabelSMOTE_genus, \"Genus\")\n",
    "svc_species = fit_linear(trainDataSMOTE_species, trainLabelSMOTE_species, \"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Loss : 0.034861047284797155\n",
      "Hamming Loss     : 0.01992665274720124\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([svc_family.predict(testData), svc_species.predict(testData), svc_genus.predict(testData)]).T\n",
    "print(\"Exact Match Loss : \", 1-exactMatch_loss(np.array(testLabel), predicted))\n",
    "print(\"Hamming Loss     : \", 1-hamming_loss(np.array(testLabel), predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On handling imbalance using SMOTE the oversampled training data seems to reduce the exact match and hamming loss.\n",
    "This evidently shows that our original data suffered from imbalance and we have effectively handled imbalance by oversampling using SMOTE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
